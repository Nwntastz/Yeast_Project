import numpy as np
import os 
import concurrent.futures
from multiprocessing import shared_memory
import itertools
#from Scripts.pwm_functions import process_batch
#from tck import TCK
#from tck.datasets import DataLoader
#import matplotlib.pyplot as plt
from sklearn.preprocessing import StandardScaler
import somoclu as som
import pycatch22 as catch22
from scipy.spatial.distance import pdist, squareform


with open(r'/mnt/c/Users/nwntas/Downloads/dists.npy',"rb") as f: 
    result=np.load(f,allow_pickle=True)

data=np.array([ [catch22.catch22_all(result[ind,row,:])["values"] for row in range(result.shape[1])] for ind in range(result.shape[0])], dtype=np.float32)

def generate_corr(profile: np.ndarray) -> np.ndarray:
    
    s=som.Somoclu(n_columns=10, n_rows=10)
    s.train(profile)
    sm=s.codebook

    n_features=profile.shape[1]
    corr_mat=np.zeros( shape=(n_features,n_features) )

    for row in range(n_features):

        feat1=sm[:,:,row].flatten()
        feat1 = ( feat1-np.mean(feat1) ) / np.std(feat1)

        for column in range(row, n_features):
            
            feat2 = sm[:,:,column].flatten()
            feat2 = ( feat2-np.mean(feat2) ) / np.std(feat2)
            
            corr_mat[row, column]=np.corrcoef(feat1, feat2)[0,1]
    
    return corr_mat

dists=np.array([generate_corr(data[gene]) for gene in range(data.shape[0])])



arr_reshaped = dists.reshape(dists.shape[0], -1)
frobenius_norm_vector = pdist(arr_reshaped, metric='euclidean')
frobenius_norm_matrix = squareform(frobenius_norm_vector)

mu,std=np.mean(frobenius_norm_matrix.flatten()), np.std(frobenius_norm_matrix.flatten())

nr=(frobenius_norm_matrix-mu)/std




### Validation of SOM clusters ###

from scipy.cluster.hierarchy import linkage, fcluster
from sklearn.metrics import silhouette_score

X_train, y_train=[],[]
links=linkage(frobenius_norm_vector, method='ward')

for num in np.linspace(1,3, 3):
    X_train+=[num]
    labels=fcluster(links, t=num, criterion='distance') #I will use a range of t in [4,12]
    y_train+=[silhouette_score(frobenius_norm_matrix, labels=labels, metric='precomputed')]

X_train, Y_train=np.array(X_train).reshape(-1,1), np.array(y_train).reshape(-1,1)

from sklearn.gaussian_process import GaussianProcessRegressor as GP
from sklearn.gaussian_process.kernels import RBF

def UCB(mean: np.array, std: np.array, coef: float)-> np.array:
    return mean + coef * std

def calculate_state(model,test_data, ucb_coef=8):
    y_mean,y_std=model.predict(X=test_data,return_std=True)
    return UCB(y_mean,y_std,ucb_coef)



def training(train_data:np.array, known_values: np.array, x_test:np.array, cycles: int = 2)-> tuple:
    #Defining the RBF kernel 
    kernel=RBF(length_scale=1)
    #Initialization of the Gaussian Process Regressor 
    gp=GP(kernel=kernel)

    #Fitting the known data
    gp.fit(X=train_data,y=known_values)
    ucb=calculate_state(gp,x_test,ucb_coef=8)
    
    #Finding the maximum point to be considered based on the UCB criterion
    next_point=x_test[np.argmax(ucb)]

    if cycles!=0:

        decay=lambda x: 0.5 * np.exp(-2 * x)

        for cycle in range(cycles):
            
            print(f"Cycle: {cycle}, {next_point}")
    
            
            #Update the training data
            train_data=np.append(train_data, next_point.reshape(1,-1),axis=0)

            labels=fcluster(links, t=next_point, criterion='distance')
            new_val=[[silhouette_score(frobenius_norm_matrix, labels=labels, metric='precomputed')*100_000]]
            known_values=np.concatenate((known_values,new_val))

            #if known_values.max()>new_val:
                #break

            #Fit the new GP
            gp.fit(X=train_data,y=known_values.reshape(-1,1))

            #Set a new test set 
            dT=decay(cycle)
            x_test=np.linspace(next_point-dT, next_point+dT, 1000).reshape(-1,1)

            #Calculate the conditional parameters for my all points that belong to my test space
            ucb=calculate_state(gp,x_test,ucb_coef=8)

            next_point=x_test[np.argmax(ucb)]
        
        return known_values.max()
    else:
        return "A cycle number is required"
    



training(X_train, Y_train*100_000, np.linspace(1,3, 3).reshape(-1,1), cycles=10)




#A small utility in order to more cleanly define the directory of a file 
def generate_path(folder: str, file: str)->str:

    '''
    This function accepts as input a system folder and a file name thought to exist within said folder.
    The function returns a full path to the desired file. If the path is not valid, it returns an error.
    '''

    full_dir=os.path.join(os.path.expanduser("~"), folder, file)
    return full_dir if os.path.exists(full_dir) else f"File {file} does not exist in Folder {folder}"

#Import the generated gene IDs
with open(rf'{generate_path("Downloads","Gene_Profiles.npy")}',"rb") as f: 
    result=np.load(f, allow_pickle=True)

with open(r'/mnt/c/Users/nwntas/Downloads/SOMs.npy',"rb") as f: 
    dists=np.load(f, allow_pickle=True)


model=TCK.TCK(G=200, C=40)
a=result[random_inds,:,:]
#noise = np.random.normal(0, 1e-2, a.shape)
#a=a+noise
b=np.moveaxis(a,1,2)
K=model.fit(b, I=2)

### Reducing gene ID dimensions ### 
res=model.predict('tr-tr')
scaler=StandardScaler()
re=scaler.fit_transform(res)

fig =  plt.figure(figsize=(6,6))
h = plt.imshow(re)
plt.title("TCK matrix (sorted)")
plt.colorbar(h)
plt.xlabel("MTS class")
plt.ylabel("MTS class")
plt.show()

from sklearn.decomposition import KernelPCA
kpca = KernelPCA(n_components=2, kernel='precomputed')
embeddings_pca = kpca.fit_transform(res)
fig =  plt.figure(figsize=(8,6))
plt.scatter(embeddings_pca[:,0], embeddings_pca[:,1], c=[1]*30 + [0]*49, s=10, cmap='tab20')
plt.title("Kernel PCA embeddings")
plt.show()

### The 1D-SAX representation ###
from tslearn.piecewise import OneD_SymbolicAggregateApproximation
from tslearn.piecewise import SymbolicAggregateApproximation


#Z-score normalization of my matrix
mu=np.mean(result,axis=-1, keepdims=True)
std=np.std(result, axis=-1, keepdims=True)
normalized=(result-mu)/std

n_sax_symbols = 8
n_segments=30
sax = SymbolicAggregateApproximation(n_segments=n_segments,
                                     alphabet_size_avg=n_sax_symbols,)


n_seg=30
one_d_sax = OneD_SymbolicAggregateApproximation(
    n_segments=n_seg,
    alphabet_size_avg=10,
    alphabet_size_slope=10,
    sigma_l=np.sqrt(0.03/(500/n_seg)))


one_d_repr=[]
for index in range(normalized.shape[0]):
    one_d_repr.append(one_d_sax.fit_transform(normalized[index,:,:,np.newaxis]))

one_d_repr=np.array(one_d_repr)
one_d_repr=np.squeeze(one_d_repr)



#This finds the SAX representations for all time series in my dataset
repr=[]
for index in range(normalized.shape[0]):
    repr.append(sax.fit_transform(normalized[index,:,:,np.newaxis]))

#This just converts the final array to the appropriate format 
repr=np.array(repr)
repr=np.squeeze(repr)


#### Distance Profile calculation function ####

def get_distance_matrix(TF: int, random_entries: np.array, repr: np.ndarray = repr, subset: int = 30, type: str = 'sax' ) -> np.ndarray:

    shm = shared_memory.SharedMemory(create=True, size=repr.nbytes)
    
    # Create a Numpy array using the shared memory
    shared_repr = np.ndarray(repr.shape, dtype=repr.dtype, buffer=shm.buf)
    np.copyto(shared_repr, repr)  # Copy data into shared memory

    #distances=np.zeros((repr.shape[0], repr.shape[0]))
    distances=np.zeros((subset,subset))
    batch_size=10

    with concurrent.futures.ProcessPoolExecutor(max_workers=2) as executor:

        futures = []

        if type=='sax':
    
            for row in range(subset): #repr.shape[0]

                batch = []

                for column in range(row, subset): #repr.shape[0]
                    
                    batch.append((random_entries[row], random_entries[column], shared_repr[:,TF,:]))

                    if len(batch) == batch_size:
                        futures.append(executor.submit(process_batch, normalized[0,:,:,np.newaxis], batch))
                        batch = []

                if batch:  # Submit any remaining pairs
                    futures.append(executor.submit(process_batch, normalized[0,:,:,np.newaxis], batch))
                
                for process in concurrent.futures.as_completed(futures):
                    
                    results=process.result()

                    for row, column, value in results:
                        distances[np.where(random_entries==row)[0][0], np.where(random_entries==column)[0][0]]=value

        elif type=='one_d_sax':  

            for row in range(subset): #repr.shape[0]

                batch = []

                for column in range(row, subset): #repr.shape[0]
                    
                    batch.append((random_entries[row], random_entries[column], shared_repr[:,TF,:]))

                    if len(batch) == batch_size:
                        futures.append(executor.submit(process_batch, normalized[0,:,:,np.newaxis], batch, 'one_d_sax'))
                        batch = []

                if batch:  # Submit any remaining pairs
                    futures.append(executor.submit(process_batch, normalized[0,:,:,np.newaxis], batch, 'one_d_sax'))
                
                for process in concurrent.futures.as_completed(futures):
                    
                    results=process.result()

                    for row, column, value in results:
                        distances[np.where(random_entries==row)[0][0], np.where(random_entries==column)[0][0]]=value
                               
    shm.close()  # Close the shared memory
    shm.unlink()  # Remove the shared memory block
   
    return distances


#### Implementing Distance calculation #####
#random_inds=np.random.choice(np.arange(one_d_repr.shape[0]), replace=False, size=100)

with open(r"C:\Users\nwntas\Downloads\Total_matrix.npy",'rb') as f:
    coexp=np.load(f, allow_pickle=True)

with open(r"/mnt/c/Users/nwntas/Downloads/Total_matrix.npy",'rb') as f:
    coexp=np.load(f, allow_pickle=True)

random_inds=np.concatenate( (np.argsort(coexp[6321,:])[-30:], np.random.choice(np.arange(result.shape[0]), replace=False, size=20)) )

random_inds=np.concatenate( (np.argsort(coexp[6321,:])[-30:],  np.argsort(coexp[6321,:])[:50]) )

dist=[]
counter=0
for TF in range(145):
    dist.append( get_distance_matrix(TF,repr=one_d_repr,random_entries=random_inds,subset=80, type='one_d_sax') )
    counter+=1
    print(f"Finished {(counter/145)*100:.2f} % of TFs", end='\r', flush=True)

distances=np.array(dist)



#This effectively summarizes each vector of matrix wise distances into a matrix to matrix distance table
dis_mat=np.mean(distances,axis=0)


dis_mat=np.linalg.norm(distances, ord=2, axis=0)


#Making my matrix symmetric
sym_di=dis_mat + dis_mat.T - np.diag(dis_mat.diagonal())


#### Plotting ####
import matplotlib.pyplot as plt
import matplotlib
import seaborn as sns

matplotlib.use('Agg')

sns.clustermap(data=frobenius_norm_matrix, method='average', cmap='viridis_r')
#plt.xticks(fontsize=3)
plt.savefig("trial_cluster_10")
plt.close()


### This is just a PCA visualization trial // Could be deleted ### 
from sklearn.decomposition import PCA
from matplotlib.lines import Line2D

mu=np.mean(sym_di,axis=-1, keepdims=True)
std=np.std(sym_di, axis=-1, keepdims=True)
norm_ds=(sym_di-mu)/std


pca = PCA(n_components=2)
tr=pca.fit_transform(norm_ds)

#Loading projection of my data
loads=pca.components_.T * np.sqrt(pca.explained_variance_)
t=norm_ds @ loads

labs=np.array([1]*30 + [0]*50)

plt.scatter(x=t[np.where(labs==1)[0],0], y=t[np.where(labs==1)[0],1], edgecolors='k', label="Over",cmap='viridis')
plt.scatter(x=t[np.where(labs==0)[0],0], y=t[np.where(labs==0)[0],1], edgecolors='k', label="Under", cmap='viridis')
sns.despine()
plt.legend()
plt.xlabel(f"PC1 ({pca.explained_variance_ratio_[0]*100:.2f}%)")
plt.ylabel(f"PC2 ({pca.explained_variance_ratio_[1]*100:.2f}%)")
plt.savefig("test")
plt.close()


#This is just to stack the summaries of the genes in an array of dimensions N_genes X N_TFs
gene_summaries=np.empty((result.shape[1]))
for id in range(result.shape[0]):
    gene_summaries=np.vstack((gene_summaries, cutoff_summary(result[id,:,:], cutoff=0.7)))


### Plotting some TF gene profiles to see how they look ###
import matplotlib.pyplot as plt
import seaborn as sns

fig, ax = plt.subplots()
sns.lineplot(data=normalized[1,1,:])
#ax.set_xlim([means.min()-0.03, means.max()+0.03])
sns.lineplot(data=one_d_sax_dataset_inv[1,:].reshape(-1,), color="k")
fig.savefig("prof1.png")

### This needs further assessment ### //Clustering
from sklearn.cluster import AgglomerativeClustering
clustering=AgglomerativeClustering().fit(gene_summaries)

set(clustering.labels_)


